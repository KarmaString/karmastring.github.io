<!DOCTYPE html><html lang="en" > <!-- The Head v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>CVPR20 - VIBE: Video Inference for Human Body Pose and Shape Estimation | Karma's Blog</title><meta name="generator" content="Jekyll v4.2.0" /><meta property="og:title" content="CVPR20 - VIBE: Video Inference for Human Body Pose and Shape Estimation" /><meta name="author" content="Karma String" /><meta property="og:locale" content="en_US" /><meta name="description" content="概述" /><meta property="og:description" content="概述" /><link rel="canonical" href="https://karmastring.github.io/posts/paper_VIBE/" /><meta property="og:url" content="https://karmastring.github.io/posts/paper_VIBE/" /><meta property="og:site_name" content="Karma’s Blog" /><meta property="og:image" content="https://karmastring.github.io/assets/img/blog/VIBE.png" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-10-05T15:15:00-04:00" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"image":"https://karmastring.github.io/assets/img/blog/VIBE.png","description":"概述","mainEntityOfPage":{"@type":"WebPage","@id":"https://karmastring.github.io/posts/paper_VIBE/"},"@type":"BlogPosting","url":"https://karmastring.github.io/posts/paper_VIBE/","headline":"CVPR20 - VIBE: Video Inference for Human Body Pose and Shape Estimation","dateModified":"2020-10-05T15:15:00-04:00","datePublished":"2020-10-05T15:15:00-04:00","author":{"@type":"Person","name":"Karma String"},"@context":"https://schema.org"}</script> <!-- The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps Generated by: https://www.favicon-generator.org/ v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT license --><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="preload" as="style" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"> <!-- CSS selector for site. Chirpy v2.3 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT Licensed --><link rel="preload" as="style" href="/assets/css/post.css"><link rel="stylesheet" href="/assets/css/post.css"><link rel="preload" as="style" href="/assets/css/lib/bootstrap-toc.min.css"><link rel="stylesheet" href="/assets/css/lib/bootstrap-toc.min.css" /><link rel="preload" as="script" href="https://cdn.jsdelivr.net/npm/jquery@3.4.1" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"> <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" async></script> <!-- JS selector for site. Chirpy v2.3 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT Licensed --> <script src="/assets/js/post.min.js" async></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script> <script src="/app.js" defer></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column"> <!-- The Side Bar v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="nav-wrapper"><div id="profile-wrapper" class="d-flex flex-column"><div id="avatar" class="d-flex justify-content-center"> <a href="/" alt="avatar"> <img src="/akane.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="profile-text mt-3"><div class="site-title"> <a href="/">Karma's Blog</a></div><div class="site-subtitle font-italic">Papers & Methodologies & Psychologies</div></div></div><ul class="nav flex-column"><li class="nav-item d-flex justify-content-center "> <a href="/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/categories/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/tags/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/archives/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/about/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></li></ul></div><div class="sidebar-bottom d-flex flex-wrap justify-content-around mt-4"> <span id="mode-toggle-wrapper"> <!-- Switch the mode between dark and light. v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <i class="mode-toggle fas fa-sun" dark-mode-invisible></i> <i class="mode-toggle fas fa-moon" light-mode-invisible></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.mode != null) { if (this.mode == ModeToggle.DARK_MODE) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.mode != null) { if (self.mode == ModeToggle.DARK_MODE) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightkMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightkMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span> <span class="icon-border"></span> <a href="https://github.com/KarmaString" target="_blank"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/" target="_blank"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:window.open('mailto:' + ['karma.string.0927','gmail.com'].join('@'))" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" > <i class="fas fa-rss"></i> </a></div></div><!-- The Top Bar v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>CVPR20 - VIBE: Video Inference for Human Body Pose and Shape Estimation</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"> <!-- Refactor the HTML structure. --> <!-- Suroundding the markdown table with '<div class="table-wrapper">. and '</div>' --> <!-- Fixed kramdown code highlight rendering: https://github.com/penibelst/jekyll-compress-html/issues/101 https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901 --><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>CVPR20 - VIBE: Video Inference for Human Body Pose and Shape Estimation</h1><div class="post-meta text-muted d-flex flex-column"><div> Posted <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Mon, Oct 5, 2020, 3:15 PM -0400" > Oct 5, 2020 <i class="unloaded">2020-10-05T15:15:00-04:00</i> </span> by <span class="author"> Karma String </span></div><div> Updated <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Sat, Feb 6, 2021, 4:18 PM -0500" > Feb 6 <i class="unloaded">2021-02-06T16:18:49-05:00</i> </span></div></div><div class="post-content"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="/assets/img/blog/VIBE.png" class="post-preview-img"><h1 id="概述">概述</h1><hr /><p>这篇paper”<em>VIBE: Video Inference for Human Body Pose and Shape Estimation</em>“是由Max Planck Institute（MPI）的人在CVPR2020上发表的，并提供了<a href="https://github.com/mkocabas/VIBE">code和pretrained model</a>。以下用<code class="language-plaintext highlighter-rouge">VIBE</code>来指代这篇paper。</p><h3 id="目标">目标</h3><p>用monocular video数据生成人的3D motion的Pose和Shape</p><h3 id="主要贡献">主要贡献：</h3><ul><li>使用了AMASS dataset对model进行了adversarial training。（这个方法其实已经是老生常谈的了，只是这种类似AMASS的3D motion capture dataset不常有。这个AMASS dataset看上去也主要是由MPI的人做的。）</li><li>在discriminator中使用了attention mechanism来weight the contribution of different frames。（使用attention也是比较常见的方法）</li><li>quantitatively地与一些temporal architectures做了些比较。</li><li>达到了state-of-the-art的结果</li></ul><h3 id="核心reference">核心reference：</h3><ul><li>[29;<code class="language-plaintext highlighter-rouge">E2ERecovery</code>] End-to-end recovery of human shape and pose [CVPR18]<ul><li>作者认为<code class="language-plaintext highlighter-rouge">E2ERecovery</code> train a single-image pose estimator using only 2D keypoints and an unpaired dataset of <strong>static</strong> 3D human shapes and poses using an adversarial training approach，从而能比较好的生成真实的<strong>static</strong>3D Human Mesh。而这篇<code class="language-plaintext highlighter-rouge">VIBE</code>正是用这个思想，并用了一个 large-scale 3D <strong>motion-capture</strong> dataset (AMASS [41]) 来生成真实的<strong>dynamic</strong> 3D Human Mesh。类似于所有的有经典adversarial net的model，<code class="language-plaintext highlighter-rouge">VIBE</code>对adversarial也有一个叙述，姑且将其记录下来：Here, given the video of a person, we train a temporal model to predict the parameters of the SMPL body model for each frame while a motion discriminator tries to distinguish between real and regressed sequences. By doing so, the regressor is encouraged to output poses that represent plausible motions through minimizing an adversarial training loss while the discriminator acts as weak supervision. The motion discriminator implicitly learns to account for the statics, physics and kinematics of the human body in motion using the ground-truth motion-capture data.</li><li>作者用了<code class="language-plaintext highlighter-rouge">E2ERecovery</code> 中regressor的结构</li></ul></li><li>[36; <code class="language-plaintext highlighter-rouge">MFLoop</code>] Learning to Reconstruct 3D Human Pose and Shape via Model-fitting in the Loop [ICCV19]<ul><li>是<code class="language-plaintext highlighter-rouge">VIBE</code>的前驱pretrained CNN网络，来从single-image生成body pose和shape的estimation。</li></ul></li><li>[40; <code class="language-plaintext highlighter-rouge">SMPL</code>] SMPL: A skinned multiperson linear model [SIGGRAPHAsia15]<ul><li>这篇paper <code class="language-plaintext highlighter-rouge">VIBE</code> 的3D Human Mesh就是基于SMPL model的</li></ul></li></ul><h1 id="实验">实验</h1><hr /><ul><li>Criteria：<ul><li>3DPW [61]</li><li>MPI-INF-3DHP [42]</li></ul></li></ul><h1 id="需要进一步调研的reference">需要进一步调研的reference</h1><hr /><ul><li>[30] Learning 3D human dynamics from video [CVPR19]<ul><li>这是这篇paper主要的baseline，与这篇paper有一样的目标。</li></ul></li><li>[29] End-to-end recovery of human shape and pose [CVPR18]<ul><li>paper中有不少思想借鉴了这篇paper，其中最重要的是：[29] train a single-image pose estimator using only 2D keypoints and an unpaired dataset of static 3D human shapes and poses using an adversarial training approach。</li></ul></li><li>[53] Human Mesh Recovery From Monocular Images via a Skeleton-Disentangled Representation [ICCV19]<ul><li>input是image或video；spatial and temporal information in a decoupling manor; self-attention; a transformer-based temporal model</li></ul></li></ul><h1 id="未解决的疑问">未解决的疑问</h1><hr /><ul><li>[29][36] 中的gender-neutral shape model是什么?</li></ul><h1 id="save-for-later">Save for later</h1><hr /><ul><li>3D Human Mesh from image(s):<ul><li>[11] Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image [ECCV16]<ul><li>始祖级</li></ul></li><li>[21] HoloPose: Holistic 3D Human Reconstruction In-The-Wild [CVPR19]<ul><li>Loss看上去很漂亮；DensePose；<strong>Euler angle limited to the convex hull to enforce attainable joint rotations</strong>.</li></ul></li><li>[25] Towards Accurate Marker-less Human Shape and Pose Estimation over Time [3DV17]<ul><li><strong>Multi-view videos</strong>；<strong>temporal prior to handle the left and right side swapping issue</strong>；silhouette</li></ul></li><li>[36] Learning to Reconstruct 3D Human Pose and Shape via Model-fitting in the Loop [ICCV19]<ul><li>combine optimization and regression approaches; 生成的动作看上去比较复杂而且极具二义性</li><li>同时也作为<code class="language-plaintext highlighter-rouge">VIBE</code>的前驱pretrained CNN网络，来从single-image生成body pose和shape的estimation。</li></ul></li><li>[38] Unite the People: Closing the loop between 3D and 2D human representations [CVPR17]</li><li>[45] Neural Body Fitting: Unifying Deep Learning and Model Based Human Pose and Shape Estimation [3DV18]<ul><li>从part segmentation到pose estimation</li></ul></li><li>[48] Learning to Estimate 3D Human Pose and Shape From a Single Color Image [CVPR18]<ul><li>最初接触的有关SMPL的文章；silhouette &amp; keypoints</li></ul></li><li>[57] Self-supervised Learning of Motion Capture [NIPS17]</li><li>[6] Exploiting Temporal Context for 3D Human Pose Estimation in the Wild [CVPR19]<ul><li>使用了temporal信息</li></ul></li></ul></li><li>3D Human Pose (skeleton)：<ul><li>[35] Self-Supervised Learning of 3D Human Pose using Multi-view Geometry [CVPR19]<ul><li>Multi-view images；no 3D ground truth (self-supervise)</li></ul></li><li>[15] Learning 3D Human Pose from Structure and Motion [ECCV18]</li><li>[24] Exploiting temporal information for 3D human pose estimation [ECCV18]</li><li>[43] Single-shot multi-person 3D pose estimation from monocular RGB [3DV18]</li><li>[44] VNect: Real-time 3D human pose estimation with a single RGB camera [SIGGRAPH17]</li><li>[49] 3D human pose estimation in video with temporal convolutions and semi-supervised training [CVPR19]</li></ul></li><li>Parametric 3D human body models:<ul><li>[4; <code class="language-plaintext highlighter-rouge">Scape</code>] Scape: Shape completion and animation of people [SIGGRAPH05]<ul><li>除了SMPL以外的另一种方法，也需要知晓一下</li></ul></li><li>[40; <code class="language-plaintext highlighter-rouge">SMPL</code>] SMPL: A skinned multiperson linear model [SIGGRAPHAsia15]<ul><li>经典方法</li></ul></li><li>[47; <code class="language-plaintext highlighter-rouge">SMPL-X</code>] Expressive Body Capture: 3D Hands, Face, and Body from a Single Image [CVPR19]<ul><li>还是MPI做的，包含了手指、脚、面部表情的SMPL加强版</li></ul></li></ul></li><li>Non-parametric body mesh reconstruction methods:<ul><li>[37] Convolutional Mesh Regression for Single-Image Human Shape Reconstruction [CVPR19]</li><li>[51] PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization [ICCV19]</li><li>DeepCap: Monocular Human Performance Capture Using Weak Supervision [CVPR20, Best Student Paper Honorable Mention]</li><li>LiveCap: Real-time Human Performance Capture from Monocular Video [TOG19]</li><li>[59] BodyNet: Volumetric Inference of 3D Human Body Shapes [ECCV18]</li></ul></li><li>GAN for modeling:<ul><li>[9] HP-GAN: Probabilistic 3D human motion prediction via GAN [CVPR18]</li><li>[20] Adversarial geometry-aware human motion prediction [ECCV18]</li><li>[2] Structured prediction helps 3D human motion modeling [ICCV19]</li></ul></li><li>2D Human Keypoint Estimation:<ul><li>[50] DeepCut: Joint subset partition and labeling for multi person pose estimation [CVPR16]</li></ul></li></ul><script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']], processEscapes: true } }); </script> <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/research/'>Research</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/paperreading/" class="post-tag no-text-decoration" >PaperReading</a> <a href="/tags/unfinished/" class="post-tag no-text-decoration" >Unfinished</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><!-- Post sharing snippet v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT License --><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=CVPR20 - VIBE: Video Inference for Human Body Pose and Shape Estimation - Karma's Blog&url=https://karmastring.github.io/posts/paper_VIBE/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=CVPR20 - VIBE: Video Inference for Human Body Pose and Shape Estimation - Karma's Blog&u=https://karmastring.github.io/posts/paper_VIBE/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=CVPR20 - VIBE: Video Inference for Human Body Pose and Shape Estimation - Karma's Blog&url=https://karmastring.github.io/posts/paper_VIBE/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><!-- The Pannel on right side (Desktop views) v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"><h3 data-toc-skip>Recent Update</h3><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/paper_E2ERecovery/">CVPR18 - End-to-end Recovery of Human Shape and Pose</a></li><li><a href="/posts/paper_SAShape/">ICCV19 - Skeleton-Aware 3D Human Shape Reconstruction From Point Clouds</a></li><li><a href="/posts/note_TED/">TED Notes</a></li><li><a href="/posts/paper_VIBE/">CVPR20 - VIBE: Video Inference for Human Body Pose and Shape Estimation</a></li><li><a href="/posts/hello/">Hello World!</a></li></ul></div><div id="access-tags"><h3 data-toc-skip>Trending Tags</h3><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/unfinished/">Unfinished</a> <a class="post-tag" href="/tags/paperreading/">PaperReading</a> <a class="post-tag" href="/tags/hello/">hello</a> <a class="post-tag" href="/tags/ted/">TED</a> <a class="post-tag" href="/tags/note/">Note</a> <a class="post-tag" href="/tags/continuous/">Continuous</a></div></div></div><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><h3 data-toc-skip class="pl-3 pt-2 mb-2">Contents</h3><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"> <!-- Recommend the other 3 posts according to the tags and categories of the current post, if the number is not enough, use the other latest posts to supplement. v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT License --><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/paper_SAShape/"><div class="card-body"> <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago small" > Feb 4 <i class="unloaded">2021-02-04T16:15:00-05:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>ICCV19 - Skeleton-Aware 3D Human Shape Reconstruction From Point Clouds</h3><div class="text-muted small"><p> 概述 这篇paper”Skeleton-Aware 3D Human Shape Reconstruction From Point Clouds“是在ICCV2019上发表的。根据作者的叙述，这可能是第一篇用点云数据并辅以SMPL Model来做3D human reconstruction的文章。以下用SAShape来指代这篇paper。 目标 用3D Point Cloud数据生...</p></div></div></a></div><div class="card"> <a href="/posts/paper_E2ERecovery/"><div class="card-body"> <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago small" > Feb 16 <i class="unloaded">2021-02-16T17:45:00-05:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>CVPR18 - End-to-end Recovery of Human Shape and Pose</h3><div class="text-muted small"><p> 概述 这篇paper”End-to-end Recovery of Human Shape and Pose“是在CVPR2018上发表的。在给出图片中人的Bounding box后， 这篇paper里的方法可以real-time地从2D image中重构出人的3D Mesh。 目标 从单张的2D RGB图像中重构出图像中人的3D mesh。作者提出的model可以使用2D-to-3...</p></div></div></a></div><div class="card"> <a href="/posts/hello/"><div class="card-body"> <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago small" > Sep 30, 2020 <i class="unloaded">2020-09-30T22:45:00-04:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Hello World!</h3><div class="text-muted small"><p> 概述 2020年9月30日的今天，我开始了这个blog来尝试记录一些见到的经验和知识。 这些经验和知识包括以下几个方面： 读一些Research Paper的记录。 我会基于对这个paper的整体评价、对我当前research比较有用的值得关注的点以及一些有价值的细节进行记录。当然这个部分也是这个blog诞生的主要意义。 ...</p></div></div></a></div></div></div><!-- Navigation buttons at the bottom of the post. v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --><div class="post-navigation d-flex justify-content-between"> <a href="/posts/hello/" class="btn btn-outline-primary"><p>Hello World!</p></a> <a href="/posts/paper_SAShape/" class="btn btn-outline-primary"><p>ICCV19 - Skeleton-Aware 3D Human Shape Reconstruction From Point Clouds</p></a></div></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('#post-wrapper img'); const observer = lozad(imgs); observer.observe(); </script> <!-- The Footer v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2021 <a href="https://github.com/KarmaString">Karma String</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy/">Chirpy</a> theme.</p></div></div></footer></div><!-- The Search results v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-xl-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/unfinished/">Unfinished</a> <a class="post-tag" href="/tags/paperreading/">PaperReading</a> <a class="post-tag" href="/tags/hello/">hello</a> <a class="post-tag" href="/tags/ted/">TED</a> <a class="post-tag" href="/tags/note/">Note</a> <a class="post-tag" href="/tags/continuous/">Continuous</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <!-- The GA snippet v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --> <script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga'); ga('create', '', 'auto'); ga('send', 'pageview'); </script> <!-- Jekyll Simple Search loader v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://karmastring.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"><div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>{categories}</div><div><i class="fa fa-tag fa-fw"></i>{tags}</div></div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>' }); </script>
